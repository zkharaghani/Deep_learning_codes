# Build and train a ConvNet in TensorFlow for a binary classification problem
# Build and train a ConvNet in TensorFlow for a multiclass classification problem

# We use the Happy House dataset, which contains images of peoples' faces. 
# The task will be to build a ConvNet that determines whether the people in
# the images are smiling or not -- because they only get to enter the house if they're smiling!


# Import necessary modules

import math
import numpy as np
import h5py
import matplotlib.pyplot as plt
from matplotlib.pyplot import imread
import scipy
from PIL import Image
import pandas as pd
import tensorflow as tf
import tensorflow.keras.layers as tfl
from tensorflow.python.framework import ops
from cnn_utils import *
from test_utils import summary, comparator

%matplotlib inline
np.random.seed(1)

# Load the Data and Split the Data into Train/Test Sets
# load_happy_dataset() is a function to read this dataset. It is imported to this module from cnn_utils module.
X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_happy_dataset() 
# Normalize image vectors
X_train = X_train_orig / 255
X_test = X_test_orig / 255
Y_train = Y_train_orig.T # Reshape
Y_test = Y_test_orig.T

def happyModel():
    """
    Implements the forward propagation for the binary classification model:
    ZEROPAD2D -> CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> FLATTEN -> DENSE
    Arguments:
    None
    Returns:
    model -- TF Keras model (object containing the information for the entire training process)
    """
    model = tf.keras.Sequential([
        tf.keras.layers.ZeroPadding2D(padding=(3,3), input_shape=(64,64,3), data_format='channels_last'),
        tf.keras.layers.Conv2D(32, (7, 7), strides = (1, 1), name = 'conv0'),
        tf.keras.layers.BatchNormalization(axis=3, name = 'bn0'),
        tf.keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0),
        tf.keras.layers.MaxPooling2D((2, 2), name='max_pool0'),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(1, activation='sigmoid', name='fc'),
    ])
    return model
    
happy_model = happyModel() # Build a sample of the function

# Compile the happy_model
happy_model.compile(optimizer='adam',
                   loss='binary_crossentropy',
                   metrics=['accuracy'])
                   
happy_model.summary()

# Fit the model
happy_model.fit(X_train, Y_train, epochs=10, batch_size=16)

# Evaluate the model
happy_model.evaluate(X_test, Y_test)


# ===========================================================================================

# The SIGNS dataset is a collection of 6 signs representing numbers from 0 to 5.

# Loading the data (signs)
X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_signs_dataset()

# Normalize image vectors
X_train = X_train_orig / 255
X_test = X_test_orig / 255
Y_train = convert_to_one_hot(Y_train_orig, 6).T
Y_test = convert_to_one_hot(Y_test_orig, 6).T

# Define the function convolutional_model
def convolutional_model(input_shape):
    """
    Implements the forward propagation for the model:
    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> DENSE
    Arguments:
    input_img -- input dataset, of shape (input_shape)
    Returns:
    model -- TF Keras model (object containing the information for the entire training process)
    """
    input_img = tf.keras.Input(shape=input_shape)
    Z1 = tf.keras.layers.Conv2D(filters = 8 , kernel_size= (4,4), strides = (1,1), padding='same')(input_img)
    A1 = tf.keras.layers.ReLU()(Z1)
    P1 = tf.keras.layers.MaxPool2D(pool_size=(8,8), strides=(8, 8), padding='same')(A1)
    Z2 = tf.keras.layers.Conv2D(filters = 16 , kernel_size= (2,2), strides = (1,1), padding='same')(P1)
    A2 = tf.keras.layers.ReLU()(Z2)
    P2 = tf.keras.layers.MaxPool2D(pool_size=(4,4), strides=(4, 4), padding='same')(A2)
    F = tf.keras.layers.Flatten()(P2)
    outputs = tf.keras.layers.Dense(units=6, activation='softmax')(F)
    model = tf.keras.Model(inputs=input_img, outputs=outputs)
    return model
    
conv_model = convolutional_model((64, 64, 3))
conv_model.summary()
conv_model.compile(optimizer='adam',
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
                  
# Train the model
train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).batch(64)
test_dataset = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).batch(64)
history = conv_model.fit(train_dataset, epochs=100, validation_data=test_dataset)
history.history
# Now visualize the loss over time using history.history
# The history.history["loss"] entry is a dictionary with as many values as epochs that the
# model was trained on. 

df_loss_acc = pd.DataFrame(history.history)
df_loss= df_loss_acc[['loss','val_loss']]
df_loss.rename(columns={'loss':'train','val_loss':'validation'},inplace=True)
df_acc= df_loss_acc[['accuracy','val_accuracy']]
df_acc.rename(columns={'accuracy':'train','val_accuracy':'validation'},inplace=True)
df_loss.plot(title='Model loss',figsize=(12,8)).set(xlabel='Epoch',ylabel='Loss')
df_acc.plot(title='Model Accuracy',figsize=(12,8)).set(xlabel='Epoch',ylabel='Accuracy')


